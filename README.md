# ScrapeMonster.tech - Big C Web Scraping Assignment

## ğŸ“Œ 1. Approach Used
This script scrapes product data from Big Câ€™s online store using **Selenium** and **Python**.  
The step-by-step process includes:

- **ğŸ” Extracting Categories:** Fetches all main categories and subcategories.
- **ğŸ”„ Handling Pagination:** Uses infinite scrolling with JavaScript to dynamically load products.
- **ğŸ“¦ Extracting Product Pages:** Captures product name, price, image, promotions, and more.
- **ğŸ›¡ï¸ Anti-Scraping Protections:**  
  - Uses a **custom user-agent** to mimic a real browser.  
  - Implements **dynamic waits** to avoid detection.  
  - Runs **headless Chrome** to improve speed.

---

## ğŸ“Š 2. Total Product Count
- **ğŸ›ï¸ Total products listed on the website:** `______`
- **âœ… Total products successfully scraped:** `______`

---

## ğŸ”„ 3. Duplicate Handling Logic
The script ensures **no duplicate products** using the following techniques:

- **ğŸ”— Checks Product URLs:** Before saving, it verifies if the product URL has already been scraped.
- **ğŸ“Œ Incremental Updates:** If a product exists in the dataset, it updates **price changes and promotions** instead of duplicating.
- **ğŸ“ JSONL Format:** The output is stored in `.jsonl` format for efficient appending without duplication.

---

## ğŸ“¦ 4. Dependencies
Ensure you have the following libraries installed:

| Library            | Version |
|--------------------|---------|
| `selenium`        | latest  |
| `pandas`          | latest  |
| `webdriver-manager` | latest  |

To install dependencies, run:

```sh
pip install selenium pandas webdriver-manager
```

## ğŸš€ 5. How to Run the Script

### âœ… Step 1: Install Dependencies
Run the following command:

```sh
pip install selenium pandas webdriver-manager
```
### âœ… Step 2: Run the Script
Execute the script using:
```sh
python scraper.py
```
### âœ… Step 3: Enter the Number of Categories
When prompted, enter:
```sh
Enter number of categories to scrape (0 for all): 11
```
This ensures the scraper runs for the 11 specified categories.
### âœ… Step 4: View and Save Data
- The script will scrape product data and save it in tops_products.jsonl.
- A summary table will be displayed in the console.
### âœ… Step 5: Verify the Scraped Data
To view the scraped data, you can run:
```sh
cat tops_products.jsonl | head -n 10
```
This will display the first 10 products in the dataset.

## ğŸš§ 6. Challenges Faced & Solutions

### **1ï¸âƒ£ CAPTCHA / Anti-Scraping Measures**
**ğŸ›‘ Problem:** Some pages block automated requests.  
**âœ… Solution:**
- Used **custom user-agents** to mimic real users.
- Added **JavaScript scrolling** instead of Seleniumâ€™s `send_keys()`.
- Implemented **randomized waits** instead of fixed delays.

---

### **2ï¸âƒ£ Pagination Issues**
**ğŸ›‘ Problem:** Some pages donâ€™t load all products at once.  
**âœ… Solution:**
- Dynamically scrolls to ensure all products are loaded.
- Uses `WebDriverWait` to detect new product loads.

---

### **3ï¸âƒ£ Website Structure Changes**
**ğŸ›‘ Problem:** HTML elements changed between runs, causing errors.  
**âœ… Solution:**
- Used `try-except` blocks to handle missing elements.
- Updated **CSS selectors** to fallback versions.

